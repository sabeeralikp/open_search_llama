{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt install lshw -y\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T04:41:00.853592Z","iopub.execute_input":"2024-11-11T04:41:00.854444Z","iopub.status.idle":"2024-11-11T04:41:58.943915Z","shell.execute_reply.started":"2024-11-11T04:41:00.854371Z","shell.execute_reply":"2024-11-11T04:41:58.942799Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  usb.ids\nThe following NEW packages will be installed:\n  lshw usb.ids\n0 upgraded, 2 newly installed, 0 to remove and 68 not upgraded.\nNeed to get 540 kB of archives.\nAfter this operation, 1675 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 lshw amd64 02.19.git.2021.06.19.996aaad9c7-2build1 [321 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb.ids all 2022.04.02-1 [219 kB]\nFetched 540 kB in 1s (560 kB/s) \u001b[0m\u001b[33m\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package lshw.\n(Reading database ... 122996 files and directories currently installed.)\nPreparing to unpack .../lshw_02.19.git.2021.06.19.996aaad9c7-2build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package usb.ids.\nPreparing to unpack .../usb.ids_2022.04.02-1_all.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking usb.ids (2022.04.02-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Setting up lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up usb.ids (2022.04.02-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Processing triggers for man-db (2.10.2-1) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J>>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n######################################################################## 100.0%                                                                5.7%#                                                        26.0%#############                                   54.9%#####################################                          67.7%######################                          67.9%###########################                         68.5%#############################################                   77.6%##########################################            86.3%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/sabeeralikp/open_search_llama.git\n%cd open_search_llama\n%pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T04:41:58.945895Z","iopub.execute_input":"2024-11-11T04:41:58.946224Z","iopub.status.idle":"2024-11-11T04:42:24.296449Z","shell.execute_reply.started":"2024-11-11T04:41:58.946189Z","shell.execute_reply":"2024-11-11T04:42:24.295207Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\n# from main import app\nimport multiprocessing\nimport asyncio\n\nauth_token = \"your ngrok auth key here\"\n\nasync def run(cmd):\n  '''\n  run is a helper function to run subcommands asynchronously.\n  '''\n  print('>>> starting', *cmd)\n  p = await asyncio.subprocess.create_subprocess_exec(\n      *cmd,\n      stdout=asyncio.subprocess.PIPE,\n      stderr=asyncio.subprocess.PIPE,\n  )\n\n  async def pipe(lines):\n    async for line in lines:\n      print(line.strip().decode('utf-8'))\n\n  await asyncio.gather(\n      pipe(p.stdout),\n      pipe(p.stderr),\n  )\n\n\nngrok.set_auth_token(auth_token)\nngrok_tunnel = ngrok.connect(8000)\nprint('Public URL:', ngrok_tunnel.public_url)\nnest_asyncio.apply()\n\nawait asyncio.gather(\n    run(['ollama', 'serve']),\n    run(['ollama', 'pull', 'llama3.2']),\n    run(['uvicorn', 'main:app', '--port', '8000']),\n)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-11-11T04:46:12.557834Z","iopub.execute_input":"2024-11-11T04:46:12.558197Z","iopub.status.idle":"2024-11-11T05:01:13.801731Z","shell.execute_reply.started":"2024-11-11T04:46:12.558165Z","shell.execute_reply":"2024-11-11T05:01:13.800190Z"}},"outputs":[{"name":"stdout","text":"Public URL: https://c069-34-55-198-29.ngrok-free.app\n>>> starting ollama serve\n>>> starting ollama pull llama3.2\n>>> starting uvicorn main:app --port 8000\nError: could not connect to ollama app, is it running?\n2024/11/11 04:46:12 routes.go:1189: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2024-11-11T04:46:12.862Z level=INFO source=images.go:755 msg=\"total blobs: 6\"\ntime=2024-11-11T04:46:12.863Z level=INFO source=images.go:762 msg=\"total unused blobs removed: 0\"\ntime=2024-11-11T04:46:12.863Z level=INFO source=routes.go:1240 msg=\"Listening on 127.0.0.1:11434 (version 0.4.1)\"\ntime=2024-11-11T04:46:12.864Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama3465041825/runners\ntime=2024-11-11T04:46:13.149Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[rocm cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12]\"\ntime=2024-11-11T04:46:13.150Z level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\ntime=2024-11-11T04:46:13.447Z level=INFO source=types.go:123 msg=\"inference compute\" id=GPU-3b32ba91-6bb9-d9f5-d458-714396d44686 library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2024-11-11T04:46:13.447Z level=INFO source=types.go:123 msg=\"inference compute\" id=GPU-01fd921c-61f3-7108-1a50-3caeb6e4909e library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\nINFO:     Started server process [385]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\ntime=2024-11-11T04:46:58.146Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-3b32ba91-6bb9-d9f5-d458-714396d44686 parallel=4 available=15720382464 required=\"3.7 GiB\"\ntime=2024-11-11T04:46:58.339Z level=INFO source=server.go:105 msg=\"system memory\" total=\"31.4 GiB\" free=\"29.9 GiB\" free_swap=\"0 B\"\ntime=2024-11-11T04:46:58.339Z level=INFO source=memory.go:343 msg=\"offload to cuda\" layers.requested=-1 layers.model=29 layers.offload=29 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"3.7 GiB\" memory.required.partial=\"3.7 GiB\" memory.required.kv=\"896.0 MiB\" memory.required.allocations=\"[3.7 GiB]\" memory.weights.total=\"2.4 GiB\" memory.weights.repeating=\"2.1 GiB\" memory.weights.nonrepeating=\"308.2 MiB\" memory.graph.full=\"424.0 MiB\" memory.graph.partial=\"570.7 MiB\"\ntime=2024-11-11T04:46:58.341Z level=INFO source=server.go:383 msg=\"starting llama server\" cmd=\"/tmp/ollama3465041825/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 2 --parallel 4 --port 40281\"\ntime=2024-11-11T04:46:58.341Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2024-11-11T04:46:58.341Z level=INFO source=server.go:562 msg=\"waiting for llama runner to start responding\"\ntime=2024-11-11T04:46:58.342Z level=INFO source=server.go:596 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2024-11-11T04:46:58.399Z level=INFO source=runner.go:863 msg=\"starting go runner\"\ntime=2024-11-11T04:46:58.399Z level=INFO source=runner.go:864 msg=system info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=2\ntime=2024-11-11T04:46:58.400Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:40281\"\nllama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\nllama_model_loader: - kv   5:                         general.size_label str              = 3B\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   8:                          llama.block_count u32              = 28\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  18:                          general.file_type u32              = 15\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\ntime=2024-11-11T04:46:58.593Z level=INFO source=server.go:596 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   58 tensors\nllama_model_loader: - type q4_K:  168 tensors\nllama_model_loader: - type q6_K:   29 tensors\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 3072\nllm_load_print_meta: n_layer          = 28\nllm_load_print_meta: n_head           = 24\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 3\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 8192\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 3B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 3.21 B\nllm_load_print_meta: model size       = 1.87 GiB (5.01 BPW)\nllm_load_print_meta: general.name     = Llama 3.2 3B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\nDevice 0: Tesla T4, compute capability 7.5, VMM: yes\nllm_load_tensors: ggml ctx size =    0.24 MiB\nllm_load_tensors: offloading 28 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 29/29 layers to GPU\nllm_load_tensors:        CPU buffer size =   308.23 MiB\nllm_load_tensors:      CUDA0 buffer size =  1918.36 MiB\nllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   896.00 MiB\nllama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     2.00 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   424.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    22.01 MiB\nllama_new_context_with_model: graph nodes  = 902\nllama_new_context_with_model: graph splits = 2\ntime=2024-11-11T04:47:00.603Z level=INFO source=server.go:601 msg=\"llama runner started in 2.26 seconds\"\n[GIN] 2024/11/11 - 04:47:01 | 200 |  3.177363358s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:47:01 | 200 |  264.178349ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:47:22 | 200 | 16.838515349s |       127.0.0.1 | POST     \"/api/chat\"\nStep: Routing Query\nStep: Routing Query to Web Search\nStep: Optimizing Query for Web Search\nStep: Searching the Web for: \"llama research papers\"\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nStep: Generating Final Response\nINFO:     2401:4900:6875:4c46:2ca7:77b:29a0:6306:0 - \"GET /run?query=List+me+a+10+research+papers+to+read+about+llama HTTP/1.1\" 200 OK\n[GIN] 2024/11/11 - 04:47:56 | 200 |  425.941493ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:47:57 | 200 |  427.161283ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:48:11 | 200 | 10.755971184s |       127.0.0.1 | POST     \"/api/chat\"\nStep: Routing Query\nStep: Routing Query to Web Search\nStep: Optimizing Query for Web Search\nStep: Searching the Web for: \"llama research papers table format\"\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nStep: Generating Final Response\nINFO:     2401:4900:6875:4c46:2ca7:77b:29a0:6306:0 - \"GET /run?query=List+me+a+10+research+papers+to+read+about+llama+in+a+table+format HTTP/1.1\" 200 OK\n[GIN] 2024/11/11 - 04:49:00 | 200 |  397.810074ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:49:00 | 200 |  474.197392ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:49:10 | 200 |   8.79895966s |       127.0.0.1 | POST     \"/api/chat\"\nStep: Routing Query\nStep: Routing Query to Web Search\nStep: Optimizing Query for Web Search\nStep: Searching the Web for: \"What is currently happening in the United States\"\nhttps://duckduckgo.com/ 202 Ratelimit\nStep: Generating Final Response\nINFO:     2401:4900:6875:4c46:2ca7:77b:29a0:6306:0 - \"GET /run?query=What+is+happening+US HTTP/1.1\" 200 OK\n[GIN] 2024/11/11 - 04:50:20 | 200 |  399.104859ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:50:20 | 200 |  420.214194ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:50:34 | 200 | 12.456086329s |       127.0.0.1 | POST     \"/api/chat\"\nStep: Routing Query\nStep: Routing Query to Web Search\nStep: Optimizing Query for Web Search\nStep: Searching the Web for: \"European Union AI policies and regulations\"\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nStep: Generating Final Response\nINFO:     2401:4900:6875:4c46:2ca7:77b:29a0:6306:0 - \"GET /run?query=What+is+current+european+union+policies+on+AI HTTP/1.1\" 200 OK\n[GIN] 2024/11/11 - 04:55:19 | 200 |   388.48428ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:55:21 | 200 |  1.920187906s |       127.0.0.1 | POST     \"/api/chat\"\nStep: Routing Query\nStep: Routing Query to Generation\nStep: Generating Final Response\nINFO:     2401:4900:6875:4c46:2ca7:77b:29a0:6306:0 - \"GET /run?query=Hello HTTP/1.1\" 200 OK\n[GIN] 2024/11/11 - 04:58:10 | 200 |  408.195161ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:58:10 | 200 |  488.782373ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:58:27 | 200 | 12.218321105s |       127.0.0.1 | POST     \"/api/chat\"\nStep: Routing Query\nStep: Routing Query to Web Search\nStep: Optimizing Query for Web Search\nStep: Searching the Web for: \"best research papers on artificial intelligence table format\"\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nStep: Generating Final Response\nINFO:     2401:4900:6875:4c46:2ca7:77b:29a0:6306:0 - \"GET /run?query=List+me+5+best+research+papers+on+AI+as+a+Table HTTP/1.1\" 200 OK\n[GIN] 2024/11/11 - 04:59:14 | 200 |  416.739203ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:59:14 | 200 |  491.050111ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/11/11 - 04:59:28 | 200 |   10.2671169s |       127.0.0.1 | POST     \"/api/chat\"\nStep: Routing Query\nStep: Routing Query to Web Search\nStep: Optimizing Query for Web Search\nStep: Searching the Web for: \"best research papers on generative artificial intelligence\"\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nhttps://duckduckgo.com/ 202 Ratelimit\nStep: Generating Final Response\nINFO:     2401:4900:6875:4c46:2ca7:77b:29a0:6306:0 - \"GET /run?query=What+are+the+best+research+papers+on+Generative+AI+as+Table HTTP/1.1\" 200 OK\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n","Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mrun.<locals>.pipe\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(lines):\n\u001b[0;32m---> 22\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:723\u001b[0m, in \u001b[0;36mStreamReader.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 723\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:524\u001b[0m, in \u001b[0;36mStreamReader.readline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaduntil(sep)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mIncompleteReadError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:616\u001b[0m, in \u001b[0;36mStreamReader.readuntil\u001b[0;34m(self, separator)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# _wait_for_data() will resume reading if stream was paused.\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreaduntil\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isep \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:501\u001b[0m, in \u001b[0;36mStreamReader._wait_for_data\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/futures.py:285\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n","Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     26\u001b[0m     pipe(p\u001b[38;5;241m.\u001b[39mstdout),\n\u001b[1;32m     27\u001b[0m     pipe(p\u001b[38;5;241m.\u001b[39mstderr),\n\u001b[1;32m     28\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPublic URL:\u001b[39m\u001b[38;5;124m'\u001b[39m, ngrok_tunnel\u001b[38;5;241m.\u001b[39mpublic_url)\n\u001b[1;32m     34\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     37\u001b[0m     run([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserve\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     38\u001b[0m     run([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpull\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3.2\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     39\u001b[0m     run([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muvicorn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain:app\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--port\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8000\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     40\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n","\u001b[0;31mCancelledError\u001b[0m: "],"ename":"CancelledError","evalue":"","output_type":"error"}],"execution_count":7}]}